Compiling MaltParser:
ant dist
java -jar ./dist/maltparser-1.7.2/maltparser-1.7.2.jar -c test -i examples/data/english_train.conll -m learn
java -jar ./dist/maltparser-1.7.2/maltparser-1.7.2.jar -c test -i examples/data/english_test.conll -o out.conll -m parse -v off

java -jar ./dist/maltparser-1.7.2/maltparser-1.7.2.jar -c test -i ../data/parser/train/0 -m learn
java -jar ./dist/maltparser-1.7.2/maltparser-1.7.2.jar -c test -i ../data/parser/dev/0 -o out.conll -m parse -v off > output


Running CoNLL data splitting tool:
num_sets indicates not only how many train and dev data sets produced (through jack-knifing), but also the ratio of training to dev data. For instance, if we have 10,000 sentences in train.dep.parse, setting num_sets to 2 will yield 5,000 training sentences and 5,000 dev sentences (x2). If we set num_sets to 10, we will have 9,000 training sentences and 1,000 dev sentences (x10). Therefore, I like to run with num_sets >= 10.
python CoNLLSplitter.py train.dep.parse [num_sets]

MegaM:
Training: ./megam.opt [multiclass, binary] data/classifier/train/0 > weights
Predicting: ./megam.opt -predict weights [multiclass, binary] data/classifier/dev/0 | head -5




Libraries used to compile our modified zPar:
You will need to install 
- Boost ( http://www.boost.org/ )
- Vorpal Wabbit ( https://github.com/JohnLangford/vowpal_wabbit )
And in order to link to the project sucessfully, the makefile will look for needed library files by calling:
-l vw  	     	     	      (libvw.a from Vorpal Wabbit)
-l allreduce 	   	      (liballreduce.a from Vorpal Wabbit)
-l boost_program_options      (libboost_program_options.a from Boost)
-l z 			      (libz.a from your system)

As a possibly helpful tip, if your boost libraries are installed in the directory /usr/local/lib, you can run the command:
$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib
To help the linker link the correct libraries. 
(tip taken from this link: http://superuser.com/questions/581624/shared-library-libvw-so-0-is-missing)


In order to compile zPar in the zpar/ directory, just run:
make generic.depparser

And to run the parser, you will need to:
train:
dist/generic.depparser/train [input Conll training data file] model 1
test:
dist/generic.depparser/depparser -c [input Conll test data file] [output file] model


 


